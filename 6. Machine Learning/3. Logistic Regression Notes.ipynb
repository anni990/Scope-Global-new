{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression:\n",
    "\n",
    "## 1. Purpose of Logistic Regression: Best Classification\n",
    "\n",
    "The goal of `Logistic Regression` is to find the best classifier (decision boundary) that effectively separates two classes by estimating the probability of a sample belonging to class 1.\n",
    "\n",
    "- Unlike Linear Regression, which predicts continuous values, Logistic Regression transforms inputs into probabilities using the **sigmoid function** and classifies them into two categories (e.g., 0 or 1).\n",
    "- It works well for binary classification problems, such as spam detection, fraud detection, and disease prediction.\n",
    "\n",
    "<img src=\"images/binary.ppm\" width='350px'>\n",
    "\n",
    "### Mathematical Representation\n",
    "For a given set of input features $X$, the logistic function is:\n",
    "\n",
    "$$\n",
    "h(X) = \\frac{1}{1 + e^{-(w^T x + b)}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h(X)$ is the probability that the point belongs to **class 1**.\n",
    "- $w$ is the **weight vector** (parameters to be learned).\n",
    "- $x$ is the **feature vector**.\n",
    "- $b$ is the **bias term**.\n",
    "\n",
    "The best classifier maximizes the separation between the two classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assumptions in Logistic Regression\n",
    "1. **Binary Output**: The target variable should have only two categories (e.g., spam or not spam).\n",
    "\n",
    "2. **No Multicollinearity**: Independent variables should not be highly correlated.\n",
    "3. **Independence of Observations**: Each observation should be independent.\n",
    "4. **Large Dataset Size**: Logistic Regression performs best with large, well-balanced datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equations of Decision Boundary: Line and Plane\n",
    "\n",
    "#### Equation of a Decision Boundary in 2D (Line)\n",
    "In a two-dimensional space, the decision boundary is a straight line given by:\n",
    "\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "Expanding it in terms of coordinates:\n",
    "\n",
    "$$\n",
    "w_1 X_1 + w_2 X_2 + b = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_1, w_2$ are the weights (coefficients) for features $X_1, X_2$.\n",
    "- $b$ is the bias term.\n",
    "\n",
    "### Equation of a Decision Boundary in Higher Dimensions (Plane/Hyperplane)\n",
    "For an $n$-dimensional space, the decision boundary is a **hyperplane**:\n",
    "\n",
    "$$\n",
    "w^T x + b = w_1 X_1 + w_2 X_2 + ... + w_n X_n + b = 0\n",
    "$$\n",
    "\n",
    "- If $w^T x + b > 0$ → Class 1\n",
    "- If $w^T x + b < 0$ → Class 0\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working of Logistic Regression: Distance Calculation\n",
    "\n",
    "To classify a point correctly, we calculate its distance from the decision boundary.\n",
    "\n",
    "### Distance from a Point to a Line (2D Space)\n",
    "For a point $(X_1, X_2)$, the perpendicular distance **$d$** from the decision boundary is given by:\n",
    "\n",
    "$$\n",
    "d = \\frac{|w^T x + b|}{\\| w \\|}\n",
    "$$\n",
    "\n",
    "### Distance from a Point to a Hyperplane (Higher Dimension)\n",
    "For a point $(X_1, X_2, ..., X_n)$ in an $n$-dimensional space, the distance is:\n",
    "\n",
    "$$\n",
    "d = \\frac{|w^T x + b|}{\\| w \\|}\n",
    "$$\n",
    "\n",
    "This distance determines whether the point is classified correctly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition for Finding the Best Classifier Line or Plane\n",
    "\n",
    "The best classifier is chosen bymaximizing the sum of correct classifications, given by the **argmax equation**:\n",
    "\n",
    "$$\n",
    "\\underset{w,b}{\\arg\\max} \\sum_{i=1}^{m} y_i (w^T x_i + b)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- We compute $w^T x_i + b$ for each training sample $x_i$.\n",
    "- The classifier that maximizes this summation is the best decision boundary.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Four Cases in Classification\n",
    "\n",
    "| Case | Condition | Distance Calculation | Correct Classification? |\n",
    "|------|-----------|----------------------|-------------------------|\n",
    "| 1. Positive point in positive region | $w^T x + b > 0$ and actual class = 1 | Distance is positive | Correct ✅ |\n",
    "| 2. Negative point in negative region | $w^T x + b < 0$ and actual class = 0 | Distance is negative | Correct ✅ |\n",
    "| 3. Negative point in positive region | $w^T x + b > 0$ but actual class = 0 | Distance is positive | Incorrect ❌ |\n",
    "| 4. Positive point in negative region | $w^T x + b < 0$ but actual class = 1 | Distance is negative | Incorrect ❌ |\n",
    "\n",
    "Thus, cases 3 and 4 indicate misclassification while cases 1 and 2 indicate correct classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Condition after training:\n",
    "After training the model, the final decision boundary equation is:\n",
    "\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "This equation **divides the feature space into two regions**:\n",
    "- Region 1 (+ class): $w^T x + b > 0$\n",
    "- Region 0 (- class): $w^T x + b < 0$\n",
    "\n",
    "The **classifier with the highest** $ \\sum y_i (w^T x_i + b) $ is chosen as the best classifier.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
